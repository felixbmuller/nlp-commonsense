{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from process_examples import load_examples, extract_terms_from_example\n",
    "from utils import load_conceptnet, normalize_conceptnet, normalize_input\n",
    "from find_shortest_path import find_word_path, render_path_verbose, search_shortest_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Implement the path search\n",
    "\n",
    "## Preprocessing of the knowledge base\n",
    "\n",
    "I am using ConceptNet for this assignment. In order to work effectively with it, I downloaded the\n",
    "knowledgebase (`conceptnet-assertions-5.7.0.csv.gz`) and transformed it. I discarded all non-English\n",
    "nodes (i.e. all nodes that don't start with `/c/en`). \n",
    "\n",
    "Afterwards I normalized all node labels by removing the prefix (`/c/en`) and possible suffixes (e.g.\n",
    "`/n`, `/v`), replacing `_` with a space, applying lowercase and applying the `WordNetLemmatizer`\n",
    "from `nltk`. Most of this normalization is not strictly needed at this point, as labels in\n",
    "ConceptNet are already normalized. However, it is still done at this point to ensure that node\n",
    "labels and input terms are normalized in exactly the same way. The normalization allows us to fuzzy\n",
    "match terms to ConceptNet nodes (e.g. because mouse and mice is normalized to the same base form),\n",
    "while still being able to find a matching node for a term in $O(1)$ (on average) using Python\n",
    "dictionaries. \"Real\" fuzzy matching like substring matching would require $O(n)$ for matching nodes\n",
    "and terms which is considerably slower.\n",
    "\n",
    "TODO fix duplicate entries issue\n",
    "\n",
    "After preprocessing, the knowledge base is represented by this six data structures:\n",
    "\n",
    "- `nodes_idx2name: list[str]` mapping node indices to normalized node names\n",
    "- `nodes_name2idx: dict[str, int]` mapping node names to indices\n",
    "- `labels_idx2name: list[str]` mapping edge label indices to labels\n",
    "- `labels_name2idx: dict[str, int]` mapping edge labels to indices\n",
    "- `adjacency_lists: dict[int, set[int]]` list of adjacent nodes for each node\n",
    "- `edge_descriptors: dict[tuple[int, int], set[EdgeDescriptor]]` maps a pair of indices to all\n",
    "  direct edges in ConceptNet between this two nodes. An edge descriptor contains the edge label\n",
    "  index, weight and row index in the CSV-file (for lookup of further attributes, if necessary)\n",
    "\n",
    "In order to enable the path search, to find reversed edges, the adjacency lists are undirected.\n",
    "`edge_descriptors` is still directed. By combining information from both data structures, the path\n",
    "visualization function can find out if an edge was traversed in original or reversed direction by\n",
    "the path finding algorithm.\n",
    "\n",
    "## Path Finding\n",
    "\n",
    "I implemented a standard breadth-first search for finding paths in the knowledge graphs. I augmented the\n",
    "algorithm to keep track of the distance between the currently traversed node and the start node.\n",
    "This information is used to stop the search after exceeding the maximum path length. I decided to\n",
    "ignore edge weights to keep the path finding simple, especially since ConceptNet often contains\n",
    "multiple edges between two nodes with different weights.\n",
    "\n",
    "TODO add code\n",
    "\n",
    "# Step 2: Use the search function and visualize the paths\n",
    "\n",
    "I used Spacy for extracting terms. I extract all tokens and noun phrases, that are not in Spacy's\n",
    "stopword list. This method allows to find all relevant terms with a high probability, even though it\n",
    "also extracts many irrelevant terms (high recall, low precision). After evaluating which extracted\n",
    "terms are not present in ConceptNet, I augmented the code to remove articles (the, a, an) from noun\n",
    "phrases. Spacy always adds articles to a noun phrase if present, while ConceptNet stores nodes\n",
    "labels without article. This significantly reduced the number of out-of-vocabulary terms.\n",
    "\n",
    "TODO reduced by how much\n",
    "\n",
    "## Maximum Path Length\n",
    "\n",
    "I experimented with different maximum paths lengths, but I found that a path length of four may\n",
    "already need a considerable amount of time while not providing very useful connections. \n",
    "\n",
    "Examples\n",
    "- `checked --HasContext--> north america --HasContext--> canada <--HasContext-- garbage` (10 sec)\n",
    "- `safe --RelatedTo--> heavy <--RelatedTo-- carry --Antonym--> baggage` (50 sec)\n",
    "- `excited <--HasSubevent-- score home run --HasPrerequisite--> play baseball --HasPrerequisite--> apply` (10 sec)\n",
    "- `harvard --IsA--> college --HasContext--> canada <--HasContext-- letter` (4 sec)\n",
    "\n",
    "Finding a path with path lengths three on the other hand usually takes less than one second and\n",
    "often provides useful connections (see the following examples). Therefore I decided to use a maximum\n",
    "path length of three.\n",
    "\n",
    "## Example Visualizations\n",
    "\n",
    "Looking for paths between all terms in the question and context on the one side and the answer\n",
    "choices on the other side is a huge number of potential paths (for example 1, there are 63\n",
    "combinations and for 39 of them a path was found). Because of this, I will only show an excerpt of\n",
    "all generated paths here (You can find all of them in the appendix).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
